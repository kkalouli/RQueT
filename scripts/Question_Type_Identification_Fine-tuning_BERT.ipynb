{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEfSbAA4QHas"
   },
   "source": [
    "# Fine-tuning BERT models on Question Type\n",
    "\n",
    "A version of this notebook was used to fine-tune the BERT models on the training set we make available. The fine-tuned models can be found in the GitHub repository. This notebook is based on the informative tutorial by Chris McCormick and Nick Ryan. (2019, July 22). BERT Fine-Tuning Tutorial with PyTorch. Retrieved from http://www.mccormickml.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tD3-T8iy8bHR",
    "outputId": "6afac902-b0f0-4518-fa51-a55cac4c7377"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z7bLKfe1omgR"
   },
   "outputs": [],
   "source": [
    "# make sure that the same seed is used all over the place for better reproducibility\n",
    "seed_val = 30\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "tf.random.set_seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_UkeC7SG2krJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>contextBefore</th>\n",
       "      <th>question</th>\n",
       "      <th>contextAfter</th>\n",
       "      <th>Merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>1114435</td>\n",
       "      <td>It triggered headlines to that effect on blogs...</td>\n",
       "      <td>So what does Media Matters really do, and what...</td>\n",
       "      <td>Joining me now in San Francisco is the founder...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1069</td>\n",
       "      <td>1114839</td>\n",
       "      <td>I believe in free press.</td>\n",
       "      <td>Did you feel like you were being used to give...</td>\n",
       "      <td>No, I didnt.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1114594</td>\n",
       "      <td>I heard that you got close to Medgar Evers fam...</td>\n",
       "      <td>And I was wondering if that was true and was t...</td>\n",
       "      <td>Well, the story is true.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1393</td>\n",
       "      <td>1115273</td>\n",
       "      <td>So some readers might wonder if this is the be...</td>\n",
       "      <td>Chrystia, you wanted to get in here?</td>\n",
       "      <td>I was just going to say, I thought actually th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1473</td>\n",
       "      <td>1115388</td>\n",
       "      <td>No.</td>\n",
       "      <td>But he goes after her and says, -How many peop...</td>\n",
       "      <td>She said that Barack Obama may have anti-Ameri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1113127</td>\n",
       "      <td>Now, you also went on MSNBCs -Hardball- to tal...</td>\n",
       "      <td>What happened there?</td>\n",
       "      <td>In that circumstance, I had appeared, as I con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>1114044</td>\n",
       "      <td>And they talked to everybody.</td>\n",
       "      <td>But were we going to go on TV with it?</td>\n",
       "      <td>No.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>1114033</td>\n",
       "      <td>Lola, Ive got 20 seconds.</td>\n",
       "      <td>Does it embarrass you at all when you say that...</td>\n",
       "      <td>It doesnt embarrass me at all, no.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1113115</td>\n",
       "      <td>Well, right now on the site there are about 12...</td>\n",
       "      <td>Will you do this again for PBS, or does it de...</td>\n",
       "      <td>It depends.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1349</td>\n",
       "      <td>1115218</td>\n",
       "      <td>I mean, that...</td>\n",
       "      <td>Why not move on?</td>\n",
       "      <td>Why not move on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                      contextBefore  \\\n",
       "862   1114435  It triggered headlines to that effect on blogs...   \n",
       "1069  1114839                           I believe in free press.   \n",
       "970   1114594  I heard that you got close to Medgar Evers fam...   \n",
       "1393  1115273  So some readers might wonder if this is the be...   \n",
       "1473  1115388                                                No.   \n",
       "30    1113127  Now, you also went on MSNBCs -Hardball- to tal...   \n",
       "585   1114044                      And they talked to everybody.   \n",
       "577   1114033                          Lola, Ive got 20 seconds.   \n",
       "21    1113115  Well, right now on the site there are about 12...   \n",
       "1349  1115218                                    I mean, that...   \n",
       "\n",
       "                                               question  \\\n",
       "862   So what does Media Matters really do, and what...   \n",
       "1069   Did you feel like you were being used to give...   \n",
       "970   And I was wondering if that was true and was t...   \n",
       "1393               Chrystia, you wanted to get in here?   \n",
       "1473  But he goes after her and says, -How many peop...   \n",
       "30                                 What happened there?   \n",
       "585              But were we going to go on TV with it?   \n",
       "577   Does it embarrass you at all when you say that...   \n",
       "21     Will you do this again for PBS, or does it de...   \n",
       "1349                                   Why not move on?   \n",
       "\n",
       "                                           contextAfter  Merged  \n",
       "862   Joining me now in San Francisco is the founder...       0  \n",
       "1069                                       No, I didnt.       1  \n",
       "970                            Well, the story is true.       0  \n",
       "1393  I was just going to say, I thought actually th...       1  \n",
       "1473  She said that Barack Obama may have anti-Ameri...       0  \n",
       "30    In that circumstance, I had appeared, as I con...       1  \n",
       "585                                                 No.       0  \n",
       "577                  It doesnt embarrass me at all, no.       1  \n",
       "21                                          It depends.       1  \n",
       "1349                                   Why not move on?       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training set\n",
    "df_train = pd.read_csv(\"rquet_trainset.csv\", delimiter='\\t', header=0)\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the golden labels with 0,1 to make ti sutiable for BERT\n",
    "df_train.replace(\"NISQ\", '0', inplace=True)\n",
    "df_train.replace(\"ISQ\", '1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XUFxhCZWUOxj"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df7f840b9254d719fef0925fabcd540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e857d559c24b64af22eb3b3cd1a342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=28, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10d72205c4c41c6b6bb0afaaeaa5d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=466062, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iuBRuloSbRk",
    "outputId": "38716045-319f-47b3-e97c-066104445cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  147\n"
     ]
    }
   ],
   "source": [
    "# just run a sample tokenization to get the max_len\n",
    "max_len = 0\n",
    "sentences_A_train = df_train.question.values\n",
    "sentences_B_train = df_train.contextBefore.values\n",
    "sentences_C_train = df_train.contextAfter.values\n",
    "for i in range(0,len(sentences_A_train), 1):\n",
    "    #Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    # NOTE: depending on what exactly you would like to fine-tune, adjust the following call:\n",
    "    # if you want to fine-tune only on the question itself, you only need: sentences_A_train[i]\n",
    "    # if you want to fine-tune on the question and its context-before, you need: sentences_A_train[i], sentences_B_train[i]\n",
    "    # if you want to fine-tune on the question and its context-after, you need: sentences_A_train[i], sentences_C_train[i]\n",
    "    input_ids = tokenizer.encode(sentences_A_train[i], sentences_C_train[i], \n",
    "                                 add_special_tokens=True)\n",
    "    #Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-jkDiViVgMV",
    "outputId": "645fada9-9d04-4d70-b406-fb2863f31884"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kkalouli/Documents/virtual_envs/env3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  And why was the mainstream media so late to the party?\n",
      "Token IDs: tensor([  101,  1998,  2339,  2001,  1996,  7731,  2865,  2061,  2397,  2000,\n",
      "         1996,  2283,  1029,   102,  5241,  2149,  2085,  1010,  8282, 10533,\n",
      "         1010,  7009,  1996,  1011,  2980,  6462,  1011,  5930,  2005,  1011,\n",
      "         1996,  2899,  2335,  1011,  1025,  9617,  5032,  9574,  1010,  2120,\n",
      "        11370,  2005,  2250,  2637,  2557,  1025,  1998,  3581,  7367,  2015,\n",
      "         3630,  1010,  2934,  1997,  2865,  1998,  2270,  3821,  2012,  1996,\n",
      "         2577,  2899,  2118,  1998,  6605,  3559,  1997,  1996,  1011,  4774,\n",
      "         2830,  1010,  1011,  2029,  2092,  2831,  2055,  2101,  1999,  1996,\n",
      "         2565,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Run proper tokenization now\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "labels_train = df_train.Merged.values\n",
    "\n",
    "for i in range(0,len(sentences_A_train), 1):\n",
    "        # NOTE: depending on what exactly you would like to fine-tune, adjust the following call:\n",
    "    # if you want to fine-tune only on the question itself, you only need: sentences_A_train[i]\n",
    "    # if you want to fine-tune on the question and its context-before, you need: sentences_A_train[i], sentences_B_train[i]\n",
    "    # if you want to fine-tune on the question and its context-after, you need: sentences_A_train[i], sentences_C_train[i]\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentences_A_train[i], sentences_C_train[i], \n",
    "                        add_special_tokens = True,\n",
    "                        truncation = True,\n",
    "                        max_length = 128,          \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask.\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "labels_train = torch.tensor(labels_train)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences_A_train[0])\n",
    "print('Token IDs:', input_ids_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OF9xQ7kFfgD4",
    "outputId": "e419ab63-5310-43f5-f271-851c52a87a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,429 training samples\n",
      "  159 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cp636LS7a3CX"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fs1WINQta3HJ",
    "outputId": "d47a9bfe-2489-44d7-b7d4-b9e3a753ccb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,  \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = True\n",
    ")\n",
    "\n",
    "# uncomment to use GPU if it is available\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sQQYaNvMb1Ua"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "o6jd6X_Zc_Mh"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8Pz0u2srggT8"
   },
   "outputs": [],
   "source": [
    "# function to run the actual training/fine-tuning of the model\n",
    "def run_train_and_val(epochs):\n",
    "  training_stats = []\n",
    "\n",
    "  # Measure the total training time for the whole run.\n",
    "  total_t0 = time.time()\n",
    "\n",
    "\n",
    "  for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "      # Perform one full pass over the training set.\n",
    "\n",
    "      print(\"\")\n",
    "      print('\\n======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "      print('\\nTraining...')\n",
    "\n",
    "      # Measure how long the training epoch takes.\n",
    "      t0 = time.time()\n",
    "\n",
    "      # Reset the total loss for this epoch.\n",
    "      total_train_loss = 0\n",
    "\n",
    "      # Put the model into training mode. \n",
    "      model.train()\n",
    "\n",
    "      # For each batch of training data...\n",
    "      for step, batch in enumerate(train_dataloader):\n",
    "          # Progress update every 40 batches.\n",
    "          if step % 40 == 0 and not step == 0:\n",
    "              # Calculate elapsed time in minutes.\n",
    "              elapsed = format_time(time.time() - t0)      \n",
    "              print('\\nBatch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "          b_input_ids = batch[0].to(device)\n",
    "          b_input_mask = batch[1].to(device)\n",
    "          b_labels = batch[2].to(device)\n",
    "\n",
    "          # Clear any previously calculated gradients before performing a\n",
    "          # backward pass.\n",
    "          model.zero_grad()        \n",
    "\n",
    "          # Perform a forward pass (evaluate the model on this training batch).\n",
    "          outputs = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels, output_hidden_states=True, output_attentions=True )\n",
    "      \n",
    "          loss = outputs.loss\n",
    "          logits = outputs.logits\n",
    "          hstates = outputs.hidden_states\n",
    "          # Accumulate the training loss over all of the batches so that we can\n",
    "          # calculate the average loss at the end. \n",
    "          total_train_loss += loss.item()\n",
    "\n",
    "          # Perform a backward pass to calculate the gradients.\n",
    "          loss.backward()\n",
    "\n",
    "          # Clip the norm of the gradients to 1.0.\n",
    "          # This is to help prevent the \"exploding gradients\" problem.\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "          # Update parameters and take a step using the computed gradient.\n",
    "          # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "          # modified based on their gradients, the learning rate, etc.\n",
    "          optimizer.step()\n",
    "\n",
    "          # Update the learning rate.\n",
    "          scheduler.step()\n",
    "\n",
    "      # Calculate the average loss over all of the batches.\n",
    "      avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "      # Measure how long this epoch took.\n",
    "      training_time = format_time(time.time() - t0)\n",
    "\n",
    "      print(\"\\n\")\n",
    "      print(\"\\nAverage training loss: {0:.2f}\".format(avg_train_loss))\n",
    "      print(\"\\nTraining epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "      print(\"\\n\")\n",
    "      print(\"\\nRunning Validation...\")\n",
    "\n",
    "      t0 = time.time()\n",
    "\n",
    "      # Put the model in evaluation mode--the dropout layers behave differently\n",
    "      # during evaluation.\n",
    "      model.eval()\n",
    "\n",
    "      # Tracking variables \n",
    "      total_eval_accuracy = 0\n",
    "      total_eval_loss = 0\n",
    "      nb_eval_steps = 0\n",
    "\n",
    "      # Evaluate data for one epoch\n",
    "      for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "          b_input_ids = batch[0].to(device)\n",
    "          b_input_mask = batch[1].to(device)\n",
    "          b_labels = batch[2].to(device)\n",
    "        \n",
    "          # Tell pytorch not to bother with constructing the compute graph during\n",
    "          # the forward pass, since this is only needed for backprop (training).\n",
    "          with torch.no_grad():        \n",
    "\n",
    "              # Forward pass, calculate logit predictions.\n",
    "              outputs = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels, output_hidden_states=True, output_attentions=True )\n",
    "            \n",
    "              loss = outputs.loss\n",
    "              logits = outputs.logits\n",
    "            \n",
    "          # Accumulate the validation loss.\n",
    "          total_eval_loss += loss.item()\n",
    "\n",
    "          # Move logits and labels to CPU\n",
    "          logits = logits.detach().cpu().numpy()\n",
    "          label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "          # Calculate the accuracy for this batch of test sentences, and\n",
    "          # accumulate it over all batches.\n",
    "          total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "      # Report the final accuracy for this validation run.\n",
    "      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "      print(\"\\nAccuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "      # Calculate the average loss over all of the batches.\n",
    "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "      # Measure how long the validation run took.\n",
    "      validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "      print(\"\\nValidation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "      print(\"\\nValidation took: {:}\".format(validation_time))\n",
    "\n",
    "      # Record all statistics from this epoch.\n",
    "      training_stats.append(\n",
    "          {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "          }\n",
    "      )\n",
    "\n",
    "  print(\"\\n\")\n",
    "  print(\"\\nTraining complete!\")\n",
    "  print(\"\\nTotal training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "  print(\"\\n\")\n",
    "  print(\"\\nTraining complete!\")\n",
    "  print(\"\\nTotal training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txRBBud7w7ht",
    "outputId": "87ff7597-4cf4-4c16-a228-01a5c8ac1e24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5b6840eb201d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create the learning rate scheduler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrun_train_and_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-184c44059bdf>\u001b[0m in \u001b[0;36mrun_train_and_val\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/virtual_envs/env3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/virtual_envs/env3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "run_train_and_val(epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txsE-hz1cjtQ",
    "outputId": "e3132311-70d4-4c5a-ccec-db2e54967305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 1,588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Run model ON TEST set\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_test = pd.read_csv(\"rquet_testset.csv\", delimiter='\\t', header=0)\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences_A_test = df_test.question.values\n",
    "sentences_B_test = df_test.contextBefore.values\n",
    "sentences_C_test = df_test.contextAfter.values\n",
    "labels_test = df_test.Merged.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "\n",
    "for i in range(0,len(sentences_A_test)):\n",
    "    # NOTE: depending on what exactly you would like to evaluate, adjust the following call:\n",
    "    # if you want to fine-tune only on the question itself, you only need: sentences_A_train[i]\n",
    "    # if you want to fine-tune on the question and its context-before, you need: sentences_A_train[i], sentences_B_train[i]\n",
    "    # if you want to fine-tune on the question and its context-after, you need: sentences_A_train[i], sentences_C_train[i]\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentences_A_test[i], sentences_C_test[i],\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,         \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt', \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids_test.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_test.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0)\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0)\n",
    "labels_test = torch.tensor(labels_test)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dMyrMiYcjvq",
    "outputId": "f86fbfb9-206d-40de-e024-67b078197f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 1,588 test sentences...\n",
      "    DONE.\n",
      "Test Accuracy: 0.9377500000000001\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids_test)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "total_test_accuracy = 0\n",
    "nb_test_steps = 0\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels, all_embeds = [], [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "  hidden_states = outputs.hidden_states\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "  i = 0\n",
    "  # go through each sentence at the second from last layer:\n",
    "  while i < len(hidden_states[-2]):\n",
    "    # following code to get the sentence embedding from the CLS (first token of each sentence)\n",
    "    sentence_embedding = hidden_states[-2][i][0]\n",
    "     # following code to get the sentence embedding as the average of all tokens\n",
    "    # get the tokens of each sentence\n",
    "    #token_vecs = hidden_states[-2][i]\n",
    "    #print (token_vecs.shape)\n",
    "    # average those tokens to get an average sentence embedding\n",
    "    #sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    #print (sentence_embedding.shape)\n",
    "    # add the embeding to the list of snetence embeddings \n",
    "    all_embeds.append(sentence_embedding)\n",
    "    i += 1\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "  tmp_test_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "  total_test_accuracy += tmp_test_accuracy\n",
    "  nb_test_steps += 1\n",
    "\n",
    "\n",
    "print('    DONE.')\n",
    "print(\"Test Accuracy: {}\".format(total_test_accuracy/nb_test_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "dbx0wWVd8Y7t",
    "outputId": "ab072aea-8c91-4a75-eac3-4f0b9453cf7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1113080</td>\n",
       "      <td>-0.624018</td>\n",
       "      <td>-0.349868</td>\n",
       "      <td>-0.382977</td>\n",
       "      <td>0.036641</td>\n",
       "      <td>-0.363415</td>\n",
       "      <td>0.023820</td>\n",
       "      <td>-0.332245</td>\n",
       "      <td>-0.255287</td>\n",
       "      <td>-0.318352</td>\n",
       "      <td>0.174629</td>\n",
       "      <td>-0.124925</td>\n",
       "      <td>-0.085574</td>\n",
       "      <td>0.125850</td>\n",
       "      <td>0.546151</td>\n",
       "      <td>0.358586</td>\n",
       "      <td>-0.262353</td>\n",
       "      <td>-0.736644</td>\n",
       "      <td>-0.391401</td>\n",
       "      <td>0.586341</td>\n",
       "      <td>0.271116</td>\n",
       "      <td>0.661377</td>\n",
       "      <td>0.148248</td>\n",
       "      <td>-0.436195</td>\n",
       "      <td>0.399141</td>\n",
       "      <td>0.398655</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>-0.212906</td>\n",
       "      <td>-0.789012</td>\n",
       "      <td>-0.264681</td>\n",
       "      <td>1.282379</td>\n",
       "      <td>-0.394703</td>\n",
       "      <td>-0.158896</td>\n",
       "      <td>-0.327694</td>\n",
       "      <td>-0.465828</td>\n",
       "      <td>-0.358759</td>\n",
       "      <td>-0.088071</td>\n",
       "      <td>0.043820</td>\n",
       "      <td>0.590825</td>\n",
       "      <td>0.237959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226612</td>\n",
       "      <td>-0.582723</td>\n",
       "      <td>-0.156315</td>\n",
       "      <td>-0.198098</td>\n",
       "      <td>-1.296372</td>\n",
       "      <td>-0.297298</td>\n",
       "      <td>-1.050315</td>\n",
       "      <td>0.036169</td>\n",
       "      <td>-0.011958</td>\n",
       "      <td>-0.025190</td>\n",
       "      <td>0.244999</td>\n",
       "      <td>0.573721</td>\n",
       "      <td>-0.100538</td>\n",
       "      <td>-0.444017</td>\n",
       "      <td>-0.188314</td>\n",
       "      <td>-0.258208</td>\n",
       "      <td>0.470525</td>\n",
       "      <td>0.210102</td>\n",
       "      <td>-0.128480</td>\n",
       "      <td>0.093418</td>\n",
       "      <td>-0.753367</td>\n",
       "      <td>-0.305138</td>\n",
       "      <td>0.198005</td>\n",
       "      <td>-0.104836</td>\n",
       "      <td>-0.978931</td>\n",
       "      <td>0.174753</td>\n",
       "      <td>-0.869373</td>\n",
       "      <td>0.431609</td>\n",
       "      <td>-0.450684</td>\n",
       "      <td>-1.275477</td>\n",
       "      <td>-0.380110</td>\n",
       "      <td>-0.372245</td>\n",
       "      <td>-0.123544</td>\n",
       "      <td>0.467679</td>\n",
       "      <td>0.132380</td>\n",
       "      <td>-0.082750</td>\n",
       "      <td>-0.174937</td>\n",
       "      <td>0.445593</td>\n",
       "      <td>0.550356</td>\n",
       "      <td>-0.129122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1113081</td>\n",
       "      <td>0.270131</td>\n",
       "      <td>-0.086376</td>\n",
       "      <td>-0.155895</td>\n",
       "      <td>-0.137007</td>\n",
       "      <td>0.305852</td>\n",
       "      <td>0.566150</td>\n",
       "      <td>-0.135364</td>\n",
       "      <td>-0.181811</td>\n",
       "      <td>0.273565</td>\n",
       "      <td>0.310873</td>\n",
       "      <td>0.351044</td>\n",
       "      <td>-0.352945</td>\n",
       "      <td>0.378537</td>\n",
       "      <td>-0.017702</td>\n",
       "      <td>-0.719168</td>\n",
       "      <td>0.302320</td>\n",
       "      <td>-0.520170</td>\n",
       "      <td>0.061731</td>\n",
       "      <td>-0.526917</td>\n",
       "      <td>0.668576</td>\n",
       "      <td>0.461040</td>\n",
       "      <td>-0.026566</td>\n",
       "      <td>0.352110</td>\n",
       "      <td>-0.148385</td>\n",
       "      <td>0.508185</td>\n",
       "      <td>0.267785</td>\n",
       "      <td>-0.016402</td>\n",
       "      <td>-0.049626</td>\n",
       "      <td>-1.111515</td>\n",
       "      <td>1.238145</td>\n",
       "      <td>-0.501640</td>\n",
       "      <td>0.376888</td>\n",
       "      <td>-0.492257</td>\n",
       "      <td>0.253948</td>\n",
       "      <td>-0.546699</td>\n",
       "      <td>-0.049612</td>\n",
       "      <td>0.248486</td>\n",
       "      <td>0.448389</td>\n",
       "      <td>0.030507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171693</td>\n",
       "      <td>-1.030645</td>\n",
       "      <td>-0.079492</td>\n",
       "      <td>0.065303</td>\n",
       "      <td>0.377806</td>\n",
       "      <td>0.093399</td>\n",
       "      <td>0.263002</td>\n",
       "      <td>-0.387891</td>\n",
       "      <td>-0.544686</td>\n",
       "      <td>-0.629705</td>\n",
       "      <td>0.287146</td>\n",
       "      <td>0.314549</td>\n",
       "      <td>0.085783</td>\n",
       "      <td>-0.538970</td>\n",
       "      <td>0.119640</td>\n",
       "      <td>-0.347085</td>\n",
       "      <td>1.125620</td>\n",
       "      <td>-0.264630</td>\n",
       "      <td>-0.589397</td>\n",
       "      <td>-0.491137</td>\n",
       "      <td>-1.186629</td>\n",
       "      <td>0.657936</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>-0.325245</td>\n",
       "      <td>-0.671269</td>\n",
       "      <td>0.758901</td>\n",
       "      <td>-0.352198</td>\n",
       "      <td>-0.469969</td>\n",
       "      <td>-1.113843</td>\n",
       "      <td>-0.140534</td>\n",
       "      <td>-0.092931</td>\n",
       "      <td>-0.493651</td>\n",
       "      <td>-0.716700</td>\n",
       "      <td>-0.913317</td>\n",
       "      <td>0.171843</td>\n",
       "      <td>-0.463660</td>\n",
       "      <td>-0.168094</td>\n",
       "      <td>0.166773</td>\n",
       "      <td>0.260086</td>\n",
       "      <td>-0.778423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1113082</td>\n",
       "      <td>0.157268</td>\n",
       "      <td>-0.372572</td>\n",
       "      <td>-0.191488</td>\n",
       "      <td>-0.215519</td>\n",
       "      <td>-0.199283</td>\n",
       "      <td>0.039909</td>\n",
       "      <td>0.258195</td>\n",
       "      <td>-0.224172</td>\n",
       "      <td>0.443172</td>\n",
       "      <td>0.338801</td>\n",
       "      <td>0.236452</td>\n",
       "      <td>-0.444333</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>-0.372710</td>\n",
       "      <td>-0.418878</td>\n",
       "      <td>0.110051</td>\n",
       "      <td>-0.503503</td>\n",
       "      <td>-0.164290</td>\n",
       "      <td>-0.344353</td>\n",
       "      <td>0.497209</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>0.029244</td>\n",
       "      <td>0.249894</td>\n",
       "      <td>-0.240682</td>\n",
       "      <td>0.746816</td>\n",
       "      <td>0.259935</td>\n",
       "      <td>-0.071188</td>\n",
       "      <td>-0.395146</td>\n",
       "      <td>-0.839992</td>\n",
       "      <td>0.841247</td>\n",
       "      <td>-0.245682</td>\n",
       "      <td>-0.107896</td>\n",
       "      <td>-0.378679</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>-0.217864</td>\n",
       "      <td>0.220568</td>\n",
       "      <td>0.126097</td>\n",
       "      <td>-0.124670</td>\n",
       "      <td>-0.329885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188724</td>\n",
       "      <td>-0.886269</td>\n",
       "      <td>-0.202307</td>\n",
       "      <td>0.116288</td>\n",
       "      <td>0.082040</td>\n",
       "      <td>0.126691</td>\n",
       "      <td>0.131280</td>\n",
       "      <td>-0.209587</td>\n",
       "      <td>-0.425719</td>\n",
       "      <td>-0.399182</td>\n",
       "      <td>0.033350</td>\n",
       "      <td>0.094186</td>\n",
       "      <td>0.154107</td>\n",
       "      <td>-0.174667</td>\n",
       "      <td>-0.141069</td>\n",
       "      <td>0.111347</td>\n",
       "      <td>0.777951</td>\n",
       "      <td>-0.526614</td>\n",
       "      <td>-0.481488</td>\n",
       "      <td>-0.584185</td>\n",
       "      <td>-0.522272</td>\n",
       "      <td>0.622787</td>\n",
       "      <td>0.168862</td>\n",
       "      <td>-0.015256</td>\n",
       "      <td>-0.585916</td>\n",
       "      <td>0.572168</td>\n",
       "      <td>-0.141197</td>\n",
       "      <td>-0.468831</td>\n",
       "      <td>-0.625200</td>\n",
       "      <td>-0.368668</td>\n",
       "      <td>0.178913</td>\n",
       "      <td>-0.801333</td>\n",
       "      <td>-0.491507</td>\n",
       "      <td>-0.779445</td>\n",
       "      <td>0.148867</td>\n",
       "      <td>-0.203044</td>\n",
       "      <td>-0.355888</td>\n",
       "      <td>0.121072</td>\n",
       "      <td>0.166022</td>\n",
       "      <td>-0.544440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1113083</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>-0.347765</td>\n",
       "      <td>-0.481607</td>\n",
       "      <td>-0.267755</td>\n",
       "      <td>-0.505731</td>\n",
       "      <td>-0.555540</td>\n",
       "      <td>-0.370373</td>\n",
       "      <td>-0.313331</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.436975</td>\n",
       "      <td>0.443511</td>\n",
       "      <td>-0.057212</td>\n",
       "      <td>0.226982</td>\n",
       "      <td>0.359988</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.039175</td>\n",
       "      <td>-0.463561</td>\n",
       "      <td>-0.471285</td>\n",
       "      <td>0.428875</td>\n",
       "      <td>-0.016978</td>\n",
       "      <td>0.041745</td>\n",
       "      <td>0.586759</td>\n",
       "      <td>-0.340973</td>\n",
       "      <td>0.280632</td>\n",
       "      <td>0.247990</td>\n",
       "      <td>-0.373314</td>\n",
       "      <td>0.194916</td>\n",
       "      <td>-0.501824</td>\n",
       "      <td>-0.507364</td>\n",
       "      <td>0.810175</td>\n",
       "      <td>-0.060308</td>\n",
       "      <td>-0.259293</td>\n",
       "      <td>-0.723265</td>\n",
       "      <td>-0.310226</td>\n",
       "      <td>-0.028130</td>\n",
       "      <td>0.249415</td>\n",
       "      <td>-0.140284</td>\n",
       "      <td>-0.035377</td>\n",
       "      <td>-0.038462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152780</td>\n",
       "      <td>-0.896757</td>\n",
       "      <td>-0.412354</td>\n",
       "      <td>0.215597</td>\n",
       "      <td>-0.316971</td>\n",
       "      <td>-0.112582</td>\n",
       "      <td>0.199049</td>\n",
       "      <td>-0.442989</td>\n",
       "      <td>-0.088435</td>\n",
       "      <td>-0.215524</td>\n",
       "      <td>0.076784</td>\n",
       "      <td>0.554259</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>-0.203523</td>\n",
       "      <td>0.034283</td>\n",
       "      <td>-0.042308</td>\n",
       "      <td>0.200920</td>\n",
       "      <td>-0.341862</td>\n",
       "      <td>-0.040199</td>\n",
       "      <td>-0.084518</td>\n",
       "      <td>-0.130297</td>\n",
       "      <td>0.110891</td>\n",
       "      <td>0.217866</td>\n",
       "      <td>0.133420</td>\n",
       "      <td>-0.332300</td>\n",
       "      <td>0.130252</td>\n",
       "      <td>-0.340088</td>\n",
       "      <td>-0.207937</td>\n",
       "      <td>-0.245231</td>\n",
       "      <td>-0.495615</td>\n",
       "      <td>-0.386236</td>\n",
       "      <td>-0.569470</td>\n",
       "      <td>-0.515972</td>\n",
       "      <td>-0.266854</td>\n",
       "      <td>-0.326021</td>\n",
       "      <td>0.188339</td>\n",
       "      <td>-0.198137</td>\n",
       "      <td>0.094648</td>\n",
       "      <td>0.341003</td>\n",
       "      <td>-0.155572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1113086</td>\n",
       "      <td>-0.275234</td>\n",
       "      <td>-0.017988</td>\n",
       "      <td>-0.059701</td>\n",
       "      <td>-0.494237</td>\n",
       "      <td>0.154583</td>\n",
       "      <td>-0.198371</td>\n",
       "      <td>-0.408940</td>\n",
       "      <td>0.304458</td>\n",
       "      <td>-0.073714</td>\n",
       "      <td>0.465623</td>\n",
       "      <td>0.656702</td>\n",
       "      <td>-0.334627</td>\n",
       "      <td>0.038798</td>\n",
       "      <td>-0.043217</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>-0.341887</td>\n",
       "      <td>-0.436489</td>\n",
       "      <td>-0.142792</td>\n",
       "      <td>-0.151310</td>\n",
       "      <td>0.254449</td>\n",
       "      <td>0.369298</td>\n",
       "      <td>0.561602</td>\n",
       "      <td>-0.263519</td>\n",
       "      <td>-0.314660</td>\n",
       "      <td>0.198678</td>\n",
       "      <td>-0.390328</td>\n",
       "      <td>-0.020252</td>\n",
       "      <td>-0.515327</td>\n",
       "      <td>-0.816254</td>\n",
       "      <td>1.156812</td>\n",
       "      <td>-0.323708</td>\n",
       "      <td>-0.153294</td>\n",
       "      <td>-0.448872</td>\n",
       "      <td>-0.186908</td>\n",
       "      <td>0.424003</td>\n",
       "      <td>0.009882</td>\n",
       "      <td>0.514401</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>0.025089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083639</td>\n",
       "      <td>-1.148470</td>\n",
       "      <td>-0.666298</td>\n",
       "      <td>-0.075288</td>\n",
       "      <td>0.174708</td>\n",
       "      <td>-0.115766</td>\n",
       "      <td>0.299112</td>\n",
       "      <td>-0.187756</td>\n",
       "      <td>-0.417176</td>\n",
       "      <td>-0.422001</td>\n",
       "      <td>0.120929</td>\n",
       "      <td>0.616716</td>\n",
       "      <td>-0.057233</td>\n",
       "      <td>-0.259054</td>\n",
       "      <td>-0.330097</td>\n",
       "      <td>-0.170284</td>\n",
       "      <td>0.351943</td>\n",
       "      <td>-0.487349</td>\n",
       "      <td>-0.045927</td>\n",
       "      <td>-0.351488</td>\n",
       "      <td>-0.454666</td>\n",
       "      <td>0.366093</td>\n",
       "      <td>-0.084367</td>\n",
       "      <td>-0.213052</td>\n",
       "      <td>-0.302986</td>\n",
       "      <td>0.016164</td>\n",
       "      <td>-0.118510</td>\n",
       "      <td>-0.780789</td>\n",
       "      <td>-0.206282</td>\n",
       "      <td>-0.348325</td>\n",
       "      <td>0.164422</td>\n",
       "      <td>-0.707787</td>\n",
       "      <td>-0.986103</td>\n",
       "      <td>-0.830617</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>-0.662303</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>-0.160354</td>\n",
       "      <td>0.427581</td>\n",
       "      <td>-0.132886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>1115541</td>\n",
       "      <td>0.158176</td>\n",
       "      <td>-0.012408</td>\n",
       "      <td>-0.074602</td>\n",
       "      <td>-0.407357</td>\n",
       "      <td>-0.424088</td>\n",
       "      <td>-0.239985</td>\n",
       "      <td>0.151819</td>\n",
       "      <td>0.192066</td>\n",
       "      <td>0.296537</td>\n",
       "      <td>0.320044</td>\n",
       "      <td>-0.044743</td>\n",
       "      <td>-0.515114</td>\n",
       "      <td>0.014144</td>\n",
       "      <td>-0.024073</td>\n",
       "      <td>-0.165450</td>\n",
       "      <td>-0.045915</td>\n",
       "      <td>-0.386078</td>\n",
       "      <td>0.379277</td>\n",
       "      <td>0.016094</td>\n",
       "      <td>-0.130435</td>\n",
       "      <td>0.082425</td>\n",
       "      <td>0.590273</td>\n",
       "      <td>-0.385874</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.266962</td>\n",
       "      <td>-0.224571</td>\n",
       "      <td>0.304705</td>\n",
       "      <td>-0.086112</td>\n",
       "      <td>-0.872228</td>\n",
       "      <td>1.146898</td>\n",
       "      <td>-0.292897</td>\n",
       "      <td>-0.267964</td>\n",
       "      <td>-0.254341</td>\n",
       "      <td>-0.179149</td>\n",
       "      <td>0.358983</td>\n",
       "      <td>0.367780</td>\n",
       "      <td>-0.067155</td>\n",
       "      <td>0.105555</td>\n",
       "      <td>-0.133914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078326</td>\n",
       "      <td>-0.693557</td>\n",
       "      <td>-0.099018</td>\n",
       "      <td>0.141188</td>\n",
       "      <td>-0.383212</td>\n",
       "      <td>-0.216709</td>\n",
       "      <td>0.347864</td>\n",
       "      <td>-0.453237</td>\n",
       "      <td>-0.373840</td>\n",
       "      <td>-0.734838</td>\n",
       "      <td>-0.045990</td>\n",
       "      <td>0.553289</td>\n",
       "      <td>-0.565405</td>\n",
       "      <td>0.009190</td>\n",
       "      <td>-0.027467</td>\n",
       "      <td>-0.167227</td>\n",
       "      <td>0.552373</td>\n",
       "      <td>-0.623909</td>\n",
       "      <td>-0.083111</td>\n",
       "      <td>-0.372948</td>\n",
       "      <td>-0.159280</td>\n",
       "      <td>-0.197918</td>\n",
       "      <td>0.037804</td>\n",
       "      <td>0.020381</td>\n",
       "      <td>-0.321624</td>\n",
       "      <td>0.530154</td>\n",
       "      <td>-0.094181</td>\n",
       "      <td>-0.099354</td>\n",
       "      <td>-0.476008</td>\n",
       "      <td>-0.558274</td>\n",
       "      <td>-0.514167</td>\n",
       "      <td>-0.230656</td>\n",
       "      <td>-0.200308</td>\n",
       "      <td>-0.336612</td>\n",
       "      <td>0.202371</td>\n",
       "      <td>-0.133938</td>\n",
       "      <td>-0.109084</td>\n",
       "      <td>-0.218883</td>\n",
       "      <td>0.179575</td>\n",
       "      <td>0.034809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>1115542</td>\n",
       "      <td>0.364921</td>\n",
       "      <td>-0.023813</td>\n",
       "      <td>-0.065696</td>\n",
       "      <td>-0.529765</td>\n",
       "      <td>-0.591624</td>\n",
       "      <td>-0.259808</td>\n",
       "      <td>-0.264047</td>\n",
       "      <td>0.303148</td>\n",
       "      <td>0.306474</td>\n",
       "      <td>-0.005742</td>\n",
       "      <td>0.512823</td>\n",
       "      <td>0.662081</td>\n",
       "      <td>0.404310</td>\n",
       "      <td>0.675601</td>\n",
       "      <td>0.077073</td>\n",
       "      <td>0.013429</td>\n",
       "      <td>-0.499592</td>\n",
       "      <td>0.139314</td>\n",
       "      <td>-0.334764</td>\n",
       "      <td>0.533451</td>\n",
       "      <td>0.481596</td>\n",
       "      <td>0.700882</td>\n",
       "      <td>0.440649</td>\n",
       "      <td>0.102899</td>\n",
       "      <td>0.292141</td>\n",
       "      <td>-0.468394</td>\n",
       "      <td>0.290697</td>\n",
       "      <td>-0.763011</td>\n",
       "      <td>-0.759818</td>\n",
       "      <td>1.409413</td>\n",
       "      <td>-0.116099</td>\n",
       "      <td>-0.063310</td>\n",
       "      <td>-0.479284</td>\n",
       "      <td>-0.437163</td>\n",
       "      <td>0.678432</td>\n",
       "      <td>0.053086</td>\n",
       "      <td>0.527613</td>\n",
       "      <td>0.325971</td>\n",
       "      <td>0.158564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096074</td>\n",
       "      <td>-0.723249</td>\n",
       "      <td>-0.783010</td>\n",
       "      <td>0.389968</td>\n",
       "      <td>-0.540631</td>\n",
       "      <td>-0.678437</td>\n",
       "      <td>-0.159369</td>\n",
       "      <td>-0.357611</td>\n",
       "      <td>-0.458857</td>\n",
       "      <td>-0.475237</td>\n",
       "      <td>0.044635</td>\n",
       "      <td>0.706214</td>\n",
       "      <td>0.440963</td>\n",
       "      <td>-0.129983</td>\n",
       "      <td>-0.091840</td>\n",
       "      <td>-0.643618</td>\n",
       "      <td>0.676874</td>\n",
       "      <td>0.401087</td>\n",
       "      <td>-0.514711</td>\n",
       "      <td>-0.075312</td>\n",
       "      <td>0.034887</td>\n",
       "      <td>-0.104844</td>\n",
       "      <td>0.200641</td>\n",
       "      <td>-0.754569</td>\n",
       "      <td>0.154211</td>\n",
       "      <td>0.526147</td>\n",
       "      <td>-0.096984</td>\n",
       "      <td>0.137251</td>\n",
       "      <td>-0.112814</td>\n",
       "      <td>-0.642588</td>\n",
       "      <td>-0.744786</td>\n",
       "      <td>-0.274648</td>\n",
       "      <td>-0.674487</td>\n",
       "      <td>-0.549828</td>\n",
       "      <td>0.123528</td>\n",
       "      <td>0.340532</td>\n",
       "      <td>-0.374444</td>\n",
       "      <td>0.304584</td>\n",
       "      <td>0.548691</td>\n",
       "      <td>0.546940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>1115543</td>\n",
       "      <td>0.018611</td>\n",
       "      <td>-0.224506</td>\n",
       "      <td>-0.253279</td>\n",
       "      <td>-0.494927</td>\n",
       "      <td>0.195240</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>-0.192719</td>\n",
       "      <td>0.477200</td>\n",
       "      <td>-0.325116</td>\n",
       "      <td>0.500421</td>\n",
       "      <td>0.477306</td>\n",
       "      <td>-0.186611</td>\n",
       "      <td>0.195496</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>-0.200533</td>\n",
       "      <td>-0.200700</td>\n",
       "      <td>-0.304475</td>\n",
       "      <td>-0.352280</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.282758</td>\n",
       "      <td>0.269599</td>\n",
       "      <td>0.544799</td>\n",
       "      <td>-0.437565</td>\n",
       "      <td>-0.225807</td>\n",
       "      <td>-0.229354</td>\n",
       "      <td>-0.186276</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>-0.866562</td>\n",
       "      <td>-0.662177</td>\n",
       "      <td>1.141256</td>\n",
       "      <td>-0.219979</td>\n",
       "      <td>-0.128027</td>\n",
       "      <td>-0.689995</td>\n",
       "      <td>-0.287059</td>\n",
       "      <td>0.303403</td>\n",
       "      <td>-0.023610</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>0.104124</td>\n",
       "      <td>-0.029865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094832</td>\n",
       "      <td>-0.465815</td>\n",
       "      <td>-0.277912</td>\n",
       "      <td>0.157774</td>\n",
       "      <td>0.031133</td>\n",
       "      <td>-0.573566</td>\n",
       "      <td>0.449876</td>\n",
       "      <td>-0.128346</td>\n",
       "      <td>-0.135731</td>\n",
       "      <td>-0.522898</td>\n",
       "      <td>-0.800787</td>\n",
       "      <td>0.888971</td>\n",
       "      <td>-0.462421</td>\n",
       "      <td>-0.577863</td>\n",
       "      <td>-0.016536</td>\n",
       "      <td>-0.150312</td>\n",
       "      <td>0.681165</td>\n",
       "      <td>-0.566920</td>\n",
       "      <td>0.093249</td>\n",
       "      <td>-0.222362</td>\n",
       "      <td>-0.296507</td>\n",
       "      <td>-0.116297</td>\n",
       "      <td>-0.166789</td>\n",
       "      <td>-0.071280</td>\n",
       "      <td>-0.311343</td>\n",
       "      <td>0.444339</td>\n",
       "      <td>-0.393851</td>\n",
       "      <td>-0.442204</td>\n",
       "      <td>-0.737572</td>\n",
       "      <td>-0.528610</td>\n",
       "      <td>-0.063539</td>\n",
       "      <td>-0.484303</td>\n",
       "      <td>-0.513213</td>\n",
       "      <td>-0.125252</td>\n",
       "      <td>-0.391486</td>\n",
       "      <td>-0.385144</td>\n",
       "      <td>-0.136336</td>\n",
       "      <td>-0.032583</td>\n",
       "      <td>0.222046</td>\n",
       "      <td>0.072368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>1115544</td>\n",
       "      <td>0.185569</td>\n",
       "      <td>0.446492</td>\n",
       "      <td>-0.016041</td>\n",
       "      <td>-0.388242</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>0.484421</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.183431</td>\n",
       "      <td>0.158993</td>\n",
       "      <td>0.546126</td>\n",
       "      <td>0.216773</td>\n",
       "      <td>-0.361427</td>\n",
       "      <td>0.134217</td>\n",
       "      <td>0.031427</td>\n",
       "      <td>-0.429827</td>\n",
       "      <td>0.110094</td>\n",
       "      <td>-0.632053</td>\n",
       "      <td>0.014069</td>\n",
       "      <td>-0.517289</td>\n",
       "      <td>0.195487</td>\n",
       "      <td>0.354109</td>\n",
       "      <td>0.230142</td>\n",
       "      <td>0.350955</td>\n",
       "      <td>-0.430518</td>\n",
       "      <td>0.206621</td>\n",
       "      <td>0.124877</td>\n",
       "      <td>-0.271989</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>-0.761377</td>\n",
       "      <td>0.962448</td>\n",
       "      <td>-0.850341</td>\n",
       "      <td>0.172674</td>\n",
       "      <td>-0.455386</td>\n",
       "      <td>0.058108</td>\n",
       "      <td>-0.499253</td>\n",
       "      <td>0.209036</td>\n",
       "      <td>0.130056</td>\n",
       "      <td>0.101431</td>\n",
       "      <td>-0.265344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171251</td>\n",
       "      <td>-0.982094</td>\n",
       "      <td>-0.134183</td>\n",
       "      <td>-0.027092</td>\n",
       "      <td>0.213803</td>\n",
       "      <td>-0.056180</td>\n",
       "      <td>-0.078376</td>\n",
       "      <td>-0.310602</td>\n",
       "      <td>-0.411686</td>\n",
       "      <td>-0.635661</td>\n",
       "      <td>0.257639</td>\n",
       "      <td>-0.016885</td>\n",
       "      <td>-0.014283</td>\n",
       "      <td>-0.558791</td>\n",
       "      <td>0.098306</td>\n",
       "      <td>-0.279386</td>\n",
       "      <td>0.762423</td>\n",
       "      <td>-0.304405</td>\n",
       "      <td>-0.577119</td>\n",
       "      <td>-0.570099</td>\n",
       "      <td>-0.737446</td>\n",
       "      <td>0.583719</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>-0.078349</td>\n",
       "      <td>-0.683977</td>\n",
       "      <td>0.419438</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>-0.629005</td>\n",
       "      <td>-1.000195</td>\n",
       "      <td>-0.447015</td>\n",
       "      <td>-0.148025</td>\n",
       "      <td>-0.561821</td>\n",
       "      <td>-0.384105</td>\n",
       "      <td>-0.648979</td>\n",
       "      <td>0.267398</td>\n",
       "      <td>-0.143278</td>\n",
       "      <td>-0.369142</td>\n",
       "      <td>0.098671</td>\n",
       "      <td>0.429855</td>\n",
       "      <td>-0.537094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>1115545</td>\n",
       "      <td>0.601863</td>\n",
       "      <td>0.057119</td>\n",
       "      <td>0.152866</td>\n",
       "      <td>-0.540972</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.323692</td>\n",
       "      <td>-0.331287</td>\n",
       "      <td>0.089789</td>\n",
       "      <td>0.430530</td>\n",
       "      <td>0.127637</td>\n",
       "      <td>0.041207</td>\n",
       "      <td>-0.317859</td>\n",
       "      <td>-0.093608</td>\n",
       "      <td>-0.032759</td>\n",
       "      <td>-0.559556</td>\n",
       "      <td>0.104361</td>\n",
       "      <td>-0.607668</td>\n",
       "      <td>0.072143</td>\n",
       "      <td>-0.486479</td>\n",
       "      <td>0.467866</td>\n",
       "      <td>0.431013</td>\n",
       "      <td>0.155591</td>\n",
       "      <td>0.168074</td>\n",
       "      <td>-0.160500</td>\n",
       "      <td>0.914012</td>\n",
       "      <td>0.085783</td>\n",
       "      <td>0.110342</td>\n",
       "      <td>0.122636</td>\n",
       "      <td>-0.947089</td>\n",
       "      <td>0.688558</td>\n",
       "      <td>-0.442267</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>-0.669453</td>\n",
       "      <td>0.386609</td>\n",
       "      <td>-0.363901</td>\n",
       "      <td>0.317470</td>\n",
       "      <td>0.276163</td>\n",
       "      <td>0.096931</td>\n",
       "      <td>-0.123754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087074</td>\n",
       "      <td>-0.968784</td>\n",
       "      <td>0.192122</td>\n",
       "      <td>-0.076502</td>\n",
       "      <td>-0.143358</td>\n",
       "      <td>-0.230865</td>\n",
       "      <td>0.053840</td>\n",
       "      <td>-0.396414</td>\n",
       "      <td>-0.735327</td>\n",
       "      <td>-0.718252</td>\n",
       "      <td>0.299419</td>\n",
       "      <td>0.283540</td>\n",
       "      <td>0.030618</td>\n",
       "      <td>-0.171467</td>\n",
       "      <td>-0.186404</td>\n",
       "      <td>-0.153446</td>\n",
       "      <td>0.950915</td>\n",
       "      <td>-0.137032</td>\n",
       "      <td>-0.284732</td>\n",
       "      <td>-0.290178</td>\n",
       "      <td>-0.842015</td>\n",
       "      <td>0.734570</td>\n",
       "      <td>0.096545</td>\n",
       "      <td>-0.106903</td>\n",
       "      <td>-0.734936</td>\n",
       "      <td>0.521128</td>\n",
       "      <td>-0.098962</td>\n",
       "      <td>-0.260414</td>\n",
       "      <td>-1.068416</td>\n",
       "      <td>-0.407685</td>\n",
       "      <td>-0.152592</td>\n",
       "      <td>-0.545774</td>\n",
       "      <td>-0.321325</td>\n",
       "      <td>-0.844999</td>\n",
       "      <td>0.200480</td>\n",
       "      <td>-0.414837</td>\n",
       "      <td>-0.121169</td>\n",
       "      <td>0.366457</td>\n",
       "      <td>0.113708</td>\n",
       "      <td>-0.465607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1588 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         0         1  ...       765       766       767\n",
       "0     1113080 -0.624018 -0.349868  ...  0.445593  0.550356 -0.129122\n",
       "1     1113081  0.270131 -0.086376  ...  0.166773  0.260086 -0.778423\n",
       "2     1113082  0.157268 -0.372572  ...  0.121072  0.166022 -0.544440\n",
       "3     1113083  0.186450 -0.347765  ...  0.094648  0.341003 -0.155572\n",
       "4     1113086 -0.275234 -0.017988  ... -0.160354  0.427581 -0.132886\n",
       "...       ...       ...       ...  ...       ...       ...       ...\n",
       "1583  1115541  0.158176 -0.012408  ... -0.218883  0.179575  0.034809\n",
       "1584  1115542  0.364921 -0.023813  ...  0.304584  0.548691  0.546940\n",
       "1585  1115543  0.018611 -0.224506  ... -0.032583  0.222046  0.072368\n",
       "1586  1115544  0.185569  0.446492  ...  0.098671  0.429855 -0.537094\n",
       "1587  1115545  0.601863  0.057119  ...  0.366457  0.113708 -0.465607\n",
       "\n",
       "[1588 rows x 769 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the predictions and true values\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# get ids of predictions and \n",
    "ids = df_test.ID.values\n",
    "\n",
    "i = 0\n",
    "# create new Dataframe to hold the fine-tuned embeds\n",
    "df_embeds = pd.DataFrame()\n",
    "\n",
    "# add each prediction to the new dataframe\n",
    "for pred in all_embeds:\n",
    "  # convert tensor to numpy array\n",
    "  numpy_pred = pred.cpu().numpy()\n",
    "  reshaped_numpy = np.reshape(numpy_pred, (1,768))\n",
    "  numpy_df = pd.DataFrame(reshaped_numpy)\n",
    "  # append\n",
    "  df_embeds = df_embeds.append(numpy_df,ignore_index=True)\n",
    "  #print (df_embeds)  \n",
    "  i +=1\n",
    "\n",
    "  \n",
    "print (i)\n",
    "# insert the ids at the front\n",
    "df_embeds.insert(0,\"ID\",ids)\n",
    "df_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49xsZAdc8uvH"
   },
   "outputs": [],
   "source": [
    "# write dataframe to csv file\n",
    "df_embeds.to_csv(\"fine-tuned_bert_embeds_on_queAndCtxAfter_testset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcPCjF9_KKyj",
    "outputId": "ed1b7332-6022-4634-a459-86c80abaeee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Save model if necessary\n",
    "model_save_name = 'que_contextAfter_fine-tuned_bert.pt'\n",
    "torch.save(model.state_dict(), model_save_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Question Identification Task - Training BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
